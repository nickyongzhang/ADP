# Audience Demography Prediction 
####   \-\- ***Webpage Feature Representation***

Keywords： Target_Advertising, Audience_Demography, Topic_Modeling
---
@author: Yong Zhang
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/groupm_log.png)

### **Acknowledgement**
>This is an instruction doc of the project what I have done during the internship period at Data and Analytics R & D, GroupM.

>Thanks to Hongming Zhou, Nganmeng Tan, Saeed Bagheri for offering me the chance to work on this project. And thanks to all the people in the department for the six month time.

-----------

### **Project Introduction**

Audience demography attributes are important for target advertising. The project aims at exploring the correlation between users’ browsing history and their demography attributes. Two new approaches, namely *Enhanced Matching & Counting (Match+)* method  and *Topic Modeling* method, have been proposed to represent web pages as numeric vectors which are used to predict demography information specific to web pages. The website-level demography information is then used for the user-level demography attribute prediction. The *Match+* method is an extension of our earlier work, namely *Matching & Counting (Match)*. The *Topic Modeling* method is a new approach which can represent web pages as compact vectors and extract the hidden topics contained in the web pages. The experiment result demonstrates that the two new representation methods demonstrate greater representing power than the original one.

More detail can be found in the **technical report** and **accompany slide** located in the **Summary** folder.

### **Project Pipeline**
The project mainly consists of seven components, namely the Demography Dataset Preparation, Web Content Crawling, Web Category Library, Match+ Representation, Pretrained Topic Models, Topic Modeling Representation, Demography Prediction. The flowchart and relationship of these components is demonstrated in the figures below:

**Flowchart of Match+ Represenation method**:
```flow
st=>start: Start
op1=>operation: Demography Dataset Preparation
op2=>operation: Web Content Crawling
io1=>subroutine: Web Content Keywords
op3=>inputoutput: Web Category Library
io2=>subroutine: Match+ Representation
op4=>operation: Demongraphy Prediction
e=>end

st->op1->op2->io1->op3->io2->op4->e
```

**Flowchart of Topic Modeling Representation method**
```flow
st=>start: Start
op1=>operation: Demography Dataset Preparation
op2=>operation: Web Content Crawling
io1=>subroutine: Web Content Keywords
op3=>inputoutput: Pretrained Topic Models
io2=>subroutine: Topic Modeling Representation
op4=>operation: Demongraphy Prediction
e=>end

st->op1->op2->io1->op3->io2->op4->e
```


----------
### **Project Files**
Each component of the project will generate certain files as we can see from the flowcharts above. I put most files as tables in a database called demography.db. The output file(s) of each component is listed below. The  bold and italic words are the names of tables in the database. 

#### **1. Demography Dataset Preparation** 

* comscore_top2000domains_weights.csv

#### **2. Web Content Crawling**

* ***url_2k***
* urlcontent.db

#### **3. Web Category Library**

* ***cat472_keywords***
* ***cat_keywords_phrase***

#### **4. Match+ Representation**

* ***MatchVector_472***
* ***MatchVector_472_phrase***
The following tables are generated by original match method
* ***cat23_keywords***
* ***MatchVector_23***

#### **5. Pretrained Topic Models**
* Pretrained models contained in '/unigram' or '/bigram' folder

#### **6. Topic Modeling Representation**
* ***LsiVector***/**LdaVector**/**RpVector**
* ***LsiBiVector***/***LdaBiVector***/***RpBiVector***
 
#### **7. Demography Prediction**
* ELM_results
* svr_results
* some figures which can be found in the '/figures' folder

----------
### **Introduction of Each Component**

#### 1. **Demography Dataset Preparation**

* *top_2000_web_domains*: The folder contains excel files which contain audience demography information of top2k domains extracted from comscore. Every excel file has no more than 15 websites. Users can find the number of visits for each demography category,i.e. male, female, etc.

* *process_comscore.py*: The number in the above excel files are number of visits, but we want the tendency score of each demography attribute of each website. This script is to achieve this objective.

* *comscore_top2000domains_weights.csv*: The output file after processing. It contains thetarget score for the regression experiments.
  
  
#### **2. Web Content Crawling**

The web page content is not available. It has to be crawled instead. As we are to crawl content contained in all kinds of websites and the html source code structures can be very different from websites to websites, it is necessary to figure out a general crawling method determining what content to crawl.

The project employed scrapy. Scrapy can be easily get by `pip install scrapy`.
To build the project, `scrapy startproject webcontent` is run in the  command shell. The new scrapy project will be under the directory where you run the command. It should look like:
```
	webcontent/
	    scrapy.cfg            # deploy configuration file

	    tutorial/             # project's Python module, you'll import your code from here
	        __init__.py

	        items.py          # project items definition file

	        pipelines.py      # project pipelines file

	        settings.py       # project settings file

	        spiders/          # a directory where you'll later put your spiders
	            __init__.py
```
To understand more about scrapy, please refer to [Scrapy Documentation]( https://doc.scrapy.org/en/latest/intro/tutorial.html). 

Data on the Web is a mess. They usually appear in diverse formats and styles.
Data may be incomplete and contains a lot of noise as well. As our task is to craw all kinds of websites, it is very critical to identify a crawling rule to determine what content to crawl. 

In implementation, different tags contained in the html source code are used to crawl corresponding information, as depicted in Figure 2.1. For example, the texts contained in the <a> tags are used as the words of links. This corresponding relationship may not be complete and exact due to the diverse organization structures of different websites. However, such relationships hold for most cases.
**Figure 2.1 Tag content relationship**
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/content_tag.png)

Combinations of four tags, namely `h`, `a`, `p`, and `img`, are crawled, totally there are 15 combinations. An alternative  is、 `visible` which crawls all the visible texts on the webpage. The `img` tag may be shorten to `i` in other places and `visible` to `v`.

To start crawling after writing the spider, run the following command in the shell.
`scrapy crawl webcontent -a tagcomb='hpai' -o url_hpaimg.json --logfile hpaimg.log`

* -a is an argument to control the tag combination
* -o is to control the output file, the crawled content will be put in the file
	'url_hpaimg.json' in this case
* --logfile save the log file in case you want to know what's happening in the
	scrapy crawling procedure cause command shell is not good place to start with.

Now let's dig into the scrapy project and expose it inside out.

* ***'webcontent\webcontent\spiders'*** directory:
	- content_spider.py: main component of the spider, codes of how to crawl.
	- comscore_top2000domains_weights.csv: Same as that in **Demography Dataset Preparation**, only urls are used in this project.
	- distinct_urls_count_more100: user level datset, contains about 170k+ unique urls which have at least 100 visits
	- utils.py: contains some utiliy functions which may be used in the 'content_spider.py'

* ***'webcontent\webcontent'*** directory:
	- items.py: define the content needed to be crawled and processed in the future. Two items fields ('url' and 'keywords') are defined in this file.
	- settings.py: configure scrapy, modify user-agent, set the crawl timedelay, proxy, etc. Change the user-agent cause some websites will refuse machine crawling. However, some websites still cannot be crawled because anti-crawl mechanism.
	- piplines.py: store the funcitons to process the crawled data, thus the crawling and processing are seperated. No need to modify in this project.


* ***'webcontent'*** directory:
	- *.log files are the scrapy crawling log. They show the footprints how websites are crawled in scrapy. e.g., hpimg.log is the log when using the tag combination hpimg
	- *.json files are the crawled results of different tag combinations. Each file is a list of dictionaries. Every dictionary has two entries: 'url' and 'keywords'. The keywords combine all the content of corresponding tags. These files are put into a database 'urlcontent.db'. Then number of crawled urls may not not identical because some the website responses are different from time to time.
	
	- 2k.log: the crawling log of the 2k website, actually hpaimg.log
	- url_2k.json: the crawled content of 2k website, actually hpaimg.json

	- 170k.log: the crawling log of the 170k website, using hpaimg  
	- url_170k.json: the crawled content of 170k website, using hpaimg

	- url_tags_table.py: transfer the *.json files into database 'urlcontent.db'

	- url_2k_table.py: transfer the 2k.json file into database 'demograpy.db'
	- url_170k_table.py: transfer the 170k.json file into database 'demograpy_170k.db'

	- urlcontent.db: content keywords of differnt tag combinations.  This database is used to select the appropriate tag combination. The experiment results demonstrate that 'hpaimg' combination is the best. If users want to do the crawling in the future, they can use 'hpaimg' directly. Actually the 'url_2k' is the crawling result using 'hpaimg'. Users may just ignore the tag combination selection procedure in the future.

Start from crawling after the 2k website, we get 1739 websites with effective content keywords after excluding the sites which don't respond and doing some filtering,  The result is store in the table ***url_2k*** of 'demography.db'. 
	
* ***wordcloud_example.py***
Finally, In order to verify the crawled content is representative of corresponding websites, word cloud is exploited to visualize the crawled content. An example is given in Demo section.

#### **3. Web Category Library**
The website category is built by crawling dmoz.org with scrapy. The 'title', 'url`,'`description' fileds under the section 'Sites' of every web page in dmoz.org are crawled. That web page's corresponding category has also been extracted. Refer to
Figure 3.1 to have a look at what a dmoz web page looks like.
**Figure 3.1 Screenshot of a dmoz webpage**
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/dmoz.PNG)
After writing the spider, start crawling by running the following command in the shell: `scrapy crawl webcat -o items.json`. The argument '-o' is to control the output file, the crawled content will be put in the file 'items.json' in this case.

Same as **Web Content Crawling**, all the files contained in the folder are listed below.

* ***'webcat\webcat\spiders'*** directory:
	- webcat_spider.py: main component of the spider, codes of how to crawl.

* ***'webcat\webcat'*** directory
	- items.py: define the content needed to be crawled and processed in the future. Four items fields are defined in this file. Therefore, in the output file, every json entry will have four keys.
	- settings.py: configure scrapy, modify user-agent, set the crawl timedelay, proxy, etc. change the user-agent cause some websites will refuse machine crawling. However, some websites still cannot be crawled because of anti-crawl mechanism.
	- piplines.py: store the funcitons to process the crawled data, thus the crawling and processing are separated. No need to modify in this project.

* ***'webcat'*** directory:
	- items.json: the output file of the crawling
	- library.py: the script building the category library by using items.json
	- utils.py: some utility functions which are used in library.py
	- catKeywords.json: the json format of the category library. Keywords are unigrams
	- catKeywords\_phrase.json: the json format of the category library. Keywords contain phrases which are connected by '\_'
	- cat\_table.py: transform the json file to database using sqlite. Therefore, two tables named ***cat_472_keywords*** and ***cat_keywords_phrase*** are to be found in demography.db

#### **4. Match+ Representation**
The first content representation method. The match+ method takes in site content keyword list ***url_2k*** and match with category library keywords ***cat472_keywords*** or ***cat_keywords_phrase*** and outputs ***MatchVector_472*** or ***MatchVector_472_phrase***. All the mentioned tables are stored in the 'demography.db'

In this folder, you can also find the materials related with the original match method. 

* ***'match_hongming'*** directory
    The original match method proposed by hongming.zhou
	- CategoryLibrary: 23 first level categroy library
	- WebCrawling_hongming: original files
	- cat_table_23.py: create the category table (***cat23_keywords***) and store in database 'demography.db'.
	- url_a.db: contains the crawling result using the original method, only 'a' tags are used, (thus store in url_a table). It also contains the category table 'cat23_keywords'. The 'cat23_keywords' has also been copied to 'demography.db'

* ***'match+_yong'*** directory
	The match+ method.
	- WebFeatureLibmatch.py: The main code of the method. It takes the website content keywords list and the category library keywords, does a matching, and outputs the content representation ***MatchVector_23***/***MatchVector_472***/***MatchVector_472_phrase*** which are store in the 'demography.db'. If doing the user level experiment, the code will store content representation matrix in the 'demography_170k.db'
	- matchvector_visulization: The top 5 features of four websites using match+ representation method.

#### **5. Pretrained Topic Models**
We train the bag of words model and topic models on a huge wikipedia corpus using `gensim`. gensim is a great tool for topic modeling. Install it by `pip install gensim`. Wikipedia provides developers access to its corpus. In this project,  all the wikepeida corpus before 01/08/2016 is crawled.

* root directory
    - enwiki-20160801-pages-articles.xml.bz2: wiki corpus
    - wikicorpus.py: Adapt from the gensim code. The changement has been highlighted in the code.
    - make\_wikicorpus.py: Adapt from the gensim code.  Build the dictionnary and bow and tfidf vectors for wiki corpus. The tfidf_model is built which will be directly used in the future. 
    - make\_wiki\_en_lsi.py:Build lsi model based on above trained dictionary and tfidf vectors
    - make\_wiki\_en\_lda.py:Build lda model based on above trained dictionary and bow vectors	
    - make\_wiki\_en\_rp.py:Build rp model based on above trained dictionary and tfidf vectors

After the above training procedure, trained dicitonary and topic models are stored in the '\unigram' or '\bigram' folder. Bigram models are trained on dictionary containing bigrams. The pretrained models may consist of several files, make sure they are used together.
Please refer to [Gensim Documentation](https://radimrehurek.com/gensim/) to know more about gensim.

#### **6. Topic Modeling Representation**
The second content representation method. This method takes in site content keyword list ***url_2k*** and goes through the bag of words model and topic models which have already been trained on the wiki coupus,  and outputs ***LsiVector***/**LdaVector**/**RpVector** if unigram dictionrary used or ***LsiBiVector***/***LdaBiVector***/***RpBiVector*** if bigram dictionary used. All the aforementioned tables are stored in the 'demography.db'

* WebFeatureBOW: The main code of the method. 

#### **7. Demography Prediction**
After the aforementioned procedure, we have obtained a multitude of representation matrices, namely ***MatchVector_23***, ***MatchVector_472***, ***LsiVector***, ***LdaVector***, ***RpVector***, ***LsiBiVector***, ***LdaBiVector***, ***RpBiVector***, ***CombVector*** (a combination of LsiVector and MatchVector_472). We test the representation power of these matrices by estimating demography attributes with several estimeaiton models. The estimation models used so far are extreme learning machine (ELM), kernel extrem learning machine (KELM), and support vector regression (SVR). 

* comscore\_top2000domains\_weights.csv: contains the target scores for for each demography attribute. Yes, it is the same file appeared above.
* comscore\_table.py: transform the comscores csv file into the database, and outputs  ***comscore*** and ***comscore_23*** (target scores for original match method) which are stored in 'demography.db'. There are two target score tables because the url lists of original method and other methods are a little bit different because they are crawled at different time points. In fact, the intersection of the two url lists should be used. However, the two sets of urls are almost the same and definitely from the same distribution. The results are still credible. 
- combine_vector_table.py: concatenate the ***LsiVector*** and ***MatchVector_472*** to form the ***CombVector***, stored in `demography.db'

* ELM_demography.py: use original ELM or kernel ELM to test the various representation methods on four demography attibute groups. All the representation features and target regression scores for each demography attribute can be found in 'demography.db'.
* svr_demography.py: use support vector regression estimation model to test performance.

* ELM_results: The experiment results of testing performance using elm and kernel elm.
* svr_results: The experiment results of testing performance using SVR.

* smooth.py: smooth the representation matrix by leveraging similarity among webpages $$\hat{p} = alpha * p + 
(1 - alpha) * n$$
$$n = \frac{1}{T}\sum_i^Ts_i$$
where $\hat{p}$ is smoothed representation, $p$ is the original representation, $n$ is the neibors representation, $s_i (i=1,\dots,T)$ is the assorted top T similar neighbors of $p$. This script has not been tested so far.

* plot.py: All the codes to plot experiment results, except those already in ElM_demography.py/svr_demography.py, are included in this script. Choose accordingly to plot.

Besides the seven main components of the whole pipiline. Several other folders can be found in this package.

* Figures: containing all the figures generated while doing the project.
* Papers: several papers about audience demography prediction.
* Summary: conating a slide and a technical report sumarizing the project.	

----------
### **Project Demo**
Let's take a simple example to see how to transform a url to a vector representation and thus obtain its demography tendency using the package. The website 'myfinance.com' is used in this demo. The **Web Content Crawling** component is used to crawl the content keywords of myfinance.com, and outputs a list of **Web Content Keywords**. After that, the keyword list is compared with already built **Web Category Library** or **Pretrained Topic Modeles**, and outputs the **Match+ Representation** or **Topic Modeling Representation**. At last, the representation vector is fed into the **Demography Prediciton** component to get its demography tendency. The flowchart is depicted as below.

**Demo Flowchart**
```flow
st=>start: myfinance.com
op1=>operation: Web Content Crawling
io1=>subroutine: Web Content Keywords
op3=>inputoutput: Web Category Library (Pretrained Topic Models)
io2=>subroutine: Match+ Representation (Topic Modeling Representation)
op4=>operation: Demongraphy Prediction
e=>end: Demography tendency

st->op1->io1->op3->io2->op4->e
```

#### **1. Crawling**
* Change line of code in content\_spider.py: `start_urls = ['http://myfinance.com']`.
* Open the cmd shell and run following commands
```
cd GroupM_package\2. Web Content Crawling\webcontent
scrapy crawl webcontent -o myfinance.json
```
The crawling procedure looks like:
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/Crawl.PNG)
Then we can get a file named 'myfinance.json' which contains one dictionary.
```
[
{"url": "myfinance.com", "keywords": "myfinance personal finance century guide financial health personal finance complicated secret saving student loans banks pay times savings easiest way cut car insurance half save living paycheck paycheck eliminate years mortgage payments reasons refi homeowner know best travel rewards cards summer easy ways debt hacks save living paycheck paycheck brilliant way start investing ridiculously easy way pay credit card debt hacks earn cashback buy reasons credit score wrong secret saving student loans banks pay times savings easiest way cut car insurance half save living paycheck paycheck eliminate years mortgage payments reasons refi homeowner know best travel rewards cards summer easy ways debt hacks save living paycheck paycheck brilliant way start investing ridiculously easy way pay credit card debt hacks earn cashback buy reasons credit score wrong"}
]
```
The word cloud visulizatin of the above content is shown below. We can see the crawled content is very informative.
**Figure 2.2 Visualization of crawled content of 'myfinance.com'**
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/myfinance.png)

* Flowing the implementation of the package, the content word keyword list is transformed into a tabel contained in the 'demography.db'. In the cmd shell, run the following commands:
```
python url_finance_table.py
```
The script`'url_finance_table.py` is same as `url_2k_table.py` except that the former uses 'myfinance.json' which the latter uses 'url_2k.json'.

#### **2. Vectorization**
If using Match+ Representation
```
cd GroupM_package\4. Match+ Representation\match+_yong
python WebFeatureLibMatch.py
```
The website 'myfinance' is now represented as a 472-dimensional vector, which is stored in database. Part of the vector looks like as follows:
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/vector_match+.PNG)
If using Topic Modeling Representation
```
cd GroupM_package\6.Topic Modeling Representation
python WebFeatureBOW.py
```
The website 'myfinance' will be represented as a 300-dimensional vector (I chose 300 topic when pretraining topic models). Part of the vector looks like as follows:
![groupm-log](https://raw.githubusercontent.com/nickzylove/ADP/master/readme_pictures/vector_bow.PNG).

#### **3. Prediction**

The gender attribute prediction results using SVR of Match+ Representation and Topic Modeling Representation are $[female, male] = [0.45, 0.55]$ and $[female, male] = [0.46, 0.54]$ respectively. The result makes sense in some way cause generally more male are interested in finance issue than female.


----------
### **More information**
The project is named Webpage Feature Representation which aims at obtaining vector representations of urls from scratch. This project plays an important role in the entire project "Audience Demography Prediction". 

At user level prediction, we can represent each url browsed by the user as a vector using either Match+ Representation or Topic Modeling Represnetation Method. Then the vectors of url list forms a user representation matrix. We can use average pooling, weighted pooling (weights determined by the visit frequency), or Bayesian pooling to get the user representation vector. As every user is representated by a vector, we can traine a classifier or regressor at the user level to predict his or her demography attributes. Another approach is that we just dump all the content key words of the all the urls browsed by the user into one user key words pool and use the two representation methods to represent the user as a vector.
